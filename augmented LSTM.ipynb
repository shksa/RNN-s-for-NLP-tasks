{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1118064588>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 3])\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.1969  0.1116 -0.2867\n",
      " -0.0516 -0.0308 -0.0018\n",
      " -0.5920  0.0826  0.0916\n",
      " -0.2875 -0.0891  0.0091\n",
      " -0.2871 -0.0840 -0.0164\n",
      "[torch.FloatTensor of size 1x5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_minibatch = Variable(torch.randn(9, 5, 4))\n",
    "\n",
    "hidden = (Variable(torch.zeros(1, 5, 3)), Variable(torch.zeros((1, 5, 3))))\n",
    "\n",
    "lstm = nn.LSTM(4, 3)\n",
    "\n",
    "out, hidden = lstm(input_minibatch, hidden)\n",
    "\n",
    "print(hidden[0].size())\n",
    "\n",
    "print(hidden[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(2, 5)\n",
      "\n",
      " 0  1\n",
      " 1  1\n",
      " 0  0\n",
      "[torch.LongTensor of size 3x2]\n",
      "\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.0084  0.5745 -0.0230 -0.5933  0.7945\n",
      "  2.2256 -0.8400 -0.4712  0.3147 -0.0600\n",
      "\n",
      "(1 ,.,.) = \n",
      "  2.2256 -0.8400 -0.4712  0.3147 -0.0600\n",
      "  2.2256 -0.8400 -0.4712  0.3147 -0.0600\n",
      "\n",
      "(2 ,.,.) = \n",
      " -0.0084  0.5745 -0.0230 -0.5933  0.7945\n",
      " -0.0084  0.5745 -0.0230 -0.5933  0.7945\n",
      "[torch.FloatTensor of size 3x2x5]\n",
      "\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.0084  0.5745 -0.0230 -0.5933  0.7945\n",
      "  2.2256 -0.8400 -0.4712  0.3147 -0.0600\n",
      "  2.2256 -0.8400 -0.4712  0.3147 -0.0600\n",
      "\n",
      "(1 ,.,.) = \n",
      "  2.2256 -0.8400 -0.4712  0.3147 -0.0600\n",
      " -0.0084  0.5745 -0.0230 -0.5933  0.7945\n",
      " -0.0084  0.5745 -0.0230 -0.5933  0.7945\n",
      "[torch.FloatTensor of size 2x3x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "print(embeds)\n",
    "lookup_tensor = torch.LongTensor([[0, 1], [1, 1], [0, 0]])\n",
    "print(lookup_tensor)\n",
    "helloandworld_embed = embeds(Variable(lookup_tensor))\n",
    "print(helloandworld_embed)\n",
    "print(helloandworld_embed.view((2, 3, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'v': 11, 'd': 3, 'o': 4, 'y': 13, 't': 7, 'a': 6, 'r': 12, 'b': 14, 'e': 2, 'T': 0, 'E': 10, 'l': 9, 'g': 5, 'k': 15, 'p': 8, 'h': 1}\n"
     ]
    }
   ],
   "source": [
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])]\n",
    "\n",
    "char_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        for char in word:\n",
    "            if char not in char_to_ix:\n",
    "                char_to_ix[char] = len(char_to_ix)\n",
    "print(char_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(len(char_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_words(sentence, char_to_ix):\n",
    "    fillchar = 16\n",
    "    length = 9\n",
    "    lookupTensor = []\n",
    "    for w in sentence:\n",
    "        idxs = [char_to_ix[char] for char in w]\n",
    "        while len(idxs) < length:\n",
    "            idxs.append(fillchar)\n",
    "        lookupTensor.append(idxs)\n",
    "    return Variable(torch.LongTensor(lookupTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "    0     1     2    16    16    16    16    16    16\n",
      "    3     4     5    16    16    16    16    16    16\n",
      "    6     7     2    16    16    16    16    16    16\n",
      "    7     1     2    16    16    16    16    16    16\n",
      "    6     8     8     9     2    16    16    16    16\n",
      "[torch.LongTensor of size 5x9]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prepare_words(training_data[0][0], char_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "   10    11     2    12    13    14     4     3    13\n",
      "   12     2     6     3    16    16    16    16    16\n",
      "    7     1     6     7    16    16    16    16    16\n",
      "   14     4     4    15    16    16    16    16    16\n",
      "[torch.LongTensor of size 4x9]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prepare_words(training_data[1][0], char_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(prepare_words(training_data[1][0], char_to_ix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in seq]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return Variable(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Everybody': 5, 'read': 6, 'ate': 2, 'the': 3, 'The': 0, 'book': 8, 'apple': 4, 'dog': 1, 'that': 7}\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, char_embedding_dim, charLSTM_hidden_dim, char_vocab_size, \n",
    "                 word_embedding_dim, wordLSTM_hidden_dim, word_vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        \n",
    "        self.charLSTM_hidden_dim = charLSTM_hidden_dim\n",
    "        self.wordLSTM_hidden_dim = wordLSTM_hidden_dim\n",
    "\n",
    "        self.char_embeddings = nn.Embedding(char_vocab_size, char_embedding_dim)\n",
    "        self.word_embeddings = nn.Embedding(word_vocab_size, word_embedding_dim)\n",
    "\n",
    "        # The LSTM takes char embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality char_hidden_dim.\n",
    "        self.charLSTM = nn.LSTM(char_embedding_dim, charLSTM_hidden_dim)\n",
    "        self.wordLSTM = nn.LSTM(word_embedding_dim + charLSTM_hidden_dim, wordLSTM_hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(wordLSTM_hidden_dim, tagset_size)\n",
    "        self.charLSTM_hidden = None\n",
    "        self.wordLSTM_hidden = self.init_wordLSTM_hidden()\n",
    "\n",
    "    def init_charLSTM_hidden(self, batch_size):\n",
    "        return (Variable(torch.zeros(1, batch_size, self.charLSTM_hidden_dim)),\n",
    "                Variable(torch.zeros(1, batch_size, self.charLSTM_hidden_dim)))\n",
    "                                              \n",
    "    def init_wordLSTM_hidden(self):\n",
    "    # Before we've done anything, we dont have any hidden state.\n",
    "    # Refer to the Pytorch documentation to see exactly\n",
    "    # why they have this dimensionality.\n",
    "    # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (Variable(torch.zeros(1, 1, wordLSTM_hidden_dim)), \n",
    "                Variable(torch.zeros(1, 1, wordLSTM_hidden_dim)))\n",
    "\n",
    "    def forward(self, char_level_sentence, word_level_sentence):\n",
    "        char_embeds = self.char_embeddings(char_level_sentence)\n",
    "        char_lstm_out, self.charLSTM_hidden = self.charLSTM(char_embeds.view(9, len(char_level_sentence), -1), self.charLSTM_hidden)\n",
    "        word_embeds = self.word_embeddings(word_level_sentence)\n",
    "        char_level_rep = self.charLSTM_hidden[0].view(len(char_level_sentence), -1)\n",
    "        wordLSTM_input = torch.cat([word_embeds, char_level_rep], 1) \n",
    "        word_lstm_out, self.wordLSTM_hidden = self.wordLSTM(wordLSTM_input.view(len(word_level_sentence), 1, -1), self.wordLSTM_hidden)\n",
    "        tag_space = self.hidden2tag(word_lstm_out.view(len(word_level_sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_embedding_dim, charLSTM_hidden_dim, char_vocab_size = 4, 6, len(char_to_ix)+1      \n",
    "word_embedding_dim, wordLSTM_hidden_dim, word_vocab_size = 8, 10, len(word_to_ix)\n",
    "tagset_size = len(tag_to_ix)\n",
    "\n",
    "model = LSTMTagger(char_embedding_dim, charLSTM_hidden_dim, char_vocab_size,\n",
    "                   word_embedding_dim, wordLSTM_hidden_dim, word_vocab_size,\n",
    "                   tagset_size)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-1.1086 -0.9924 -1.2063\n",
      "-1.0558 -1.0907 -1.1517\n",
      "-0.9301 -1.2282 -1.1626\n",
      "-0.9934 -1.2685 -1.0543\n",
      "[torch.FloatTensor of size 4x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i\n",
    "char_level_sentence = prepare_words(training_data[1][0], char_to_ix)\n",
    "word_level_sentence = prepare_sequence(training_data[1][0], word_to_ix)\n",
    "\n",
    "model.wordLSTM_hidden = model.init_wordLSTM_hidden()\n",
    "model.charLSTM_hidden = model.init_charLSTM_hidden(len(char_level_sentence))\n",
    "\n",
    "tag_scores = model(char_level_sentence, word_level_sentence)\n",
    "print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.wordLSTM_hidden = model.init_wordLSTM_hidden()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Variables of word indices.\n",
    "        char_level_sentence = prepare_words(sentence, char_to_ix)\n",
    "        word_level_sentence = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        model.charLSTM_hidden = model.init_charLSTM_hidden(len(char_level_sentence))\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(char_level_sentence, word_level_sentence)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.0037 -5.6912 -7.9595\n",
      "-6.6330 -0.0037 -6.0447\n",
      "-6.6286 -5.8357 -0.0043\n",
      "-0.0010 -7.6598 -7.5149\n",
      "-7.0904 -0.0013 -7.6986\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "\n",
      " 0\n",
      " 1\n",
      " 2\n",
      " 0\n",
      " 1\n",
      "[torch.LongTensor of size 5x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See what the scores are after training\n",
    "char_level_sentence = prepare_words(training_data[0][0], char_to_ix)\n",
    "word_level_sentence = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "\n",
    "model.wordLSTM_hidden = model.init_wordLSTM_hidden()\n",
    "model.charLSTM_hidden = model.init_charLSTM_hidden(len(char_level_sentence))\n",
    "\n",
    "tag_scores = model(char_level_sentence, word_level_sentence)\n",
    "print(tag_scores)\n",
    "_, preds = torch.max(tag_scores.data, 1)\n",
    "print(preds)\n",
    "# The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "#  for word i. The predicted tag is the maximum scoring tag.\n",
    "# Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "# since 0 is index of the maximum value of row 1,\n",
    "# 1 is the index of maximum value of row 2, etc.\n",
    "# Which is DET NOUN VERB DET NOUN, the correct sequence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-6.4791 -0.0026 -6.8495\n",
      "-7.4138 -6.5673 -0.0020\n",
      "-0.0007 -7.7771 -8.2438\n",
      "-7.0292 -0.0020 -6.7628\n",
      "[torch.FloatTensor of size 4x3]\n",
      "\n",
      "\n",
      " 1\n",
      " 2\n",
      " 0\n",
      " 1\n",
      "[torch.LongTensor of size 4x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See what the scores are after training\n",
    "char_level_sentence = prepare_words(training_data[1][0], char_to_ix)\n",
    "word_level_sentence = prepare_sequence(training_data[1][0], word_to_ix)\n",
    "\n",
    "model.wordLSTM_hidden = model.init_wordLSTM_hidden()\n",
    "model.charLSTM_hidden = model.init_charLSTM_hidden(len(char_level_sentence))\n",
    "\n",
    "tag_scores = model(char_level_sentence, word_level_sentence)\n",
    "print(tag_scores)\n",
    "_, preds = torch.max(tag_scores.data, 1)\n",
    "print(preds)\n",
    "# The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "#  for word i. The predicted tag is the maximum scoring tag.\n",
    "# Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "# since 0 is index of the maximum value of row 1,\n",
    "# 1 is the index of maximum value of row 2, etc.\n",
    "# Which is DET NOUN VERB DET NOUN, the correct sequence!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
